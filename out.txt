>>> from load import get_data, get_test
>>> from keras.models import Sequential
>>> from keras.layers import Dense
>>> from keras.layers import LSTM
>>> from keras.layers.embeddings import Embedding
>>> from keras.preprocessing import sequence
>>> # set value
... vocabulary_size = 5000
>>> max_words = 150
>>> embedding_size=32
>>> batch_size = 64
>>> num_epochs = 3
>>>
>>> # load data
... X_train, y_train = get_data('train.csv', vocabulary_size)
>>> # X_test = get_test('test.csv', vocabulary_size)
... X_test, y_test = X_train[:batch_size], y_train[:batch_size]
>>> X_train, y_train = X_train[batch_size:], y_train[batch_size:]
>>> print(X_train.shape, y_train.shape)
(1025, 150) (1025,)
>>> print(X_test.shape)
(64, 150)
>>>
>>> # creat model
... embedding_size=32
>>> model = Sequential()
>>> # model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))
... # model.add(LSTM(100))
... model.add(Dense(100, activation='relu',input_shape=(150,)))
>>> model.add(Dense(100, activation='relu'))
>>>
>>> model.add(Dense(100, activation='relu'))
>>> model.add(Dense(10, activation='relu'))
>>> model.add(Dense(1, activation='sigmoid'))
>>> model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
>>> print(model.summary())
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
dense_66 (Dense)             (None, 100)               15100
_________________________________________________________________
dense_67 (Dense)             (None, 100)               10100
_________________________________________________________________
dense_68 (Dense)             (None, 100)               10100
_________________________________________________________________
dense_69 (Dense)             (None, 10)                1010
_________________________________________________________________
dense_70 (Dense)             (None, 1)                 11
=================================================================
Total params: 36,321
Trainable params: 36,321
Non-trainable params: 0
_________________________________________________________________
None
>>>
>>> X_valid, y_valid = X_train[:batch_size], y_train[:batch_size]
>>> X_train2, y_train2 = X_train[batch_size:], y_train[batch_size:]
>>> model.fit(X_train2, y_train2, validation_data=(X_valid, y_valid), batch_size=batch_size, epochs=num_epochs)
Train on 961 samples, validate on 64 samples
Epoch 1/3
961/961 [==============================] - 0s 475us/step - loss: 6.5691 - acc: 0.5806 - val_loss: 7.2239 - val_acc: 0.5469
Epoch 2/3
961/961 [==============================] - 0s 39us/step - loss: 6.1483 - acc: 0.6139 - val_loss: 6.9748 - val_acc: 0.5625
Epoch 3/3
961/961 [==============================] - 0s 49us/step - loss: 6.1292 - acc: 0.6160 - val_loss: 4.2515 - val_acc: 0.7344
<keras.callbacks.History object at 0x7fabd3dc9588>
>>> scores = model.evaluate(X_test, y_test, verbose=0)
>>> print('Test accuracy:', scores[1])
Test accuracy: 0.75
>>> out = model.predict(X_test)
>>> print(out)